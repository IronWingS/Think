试用期期间，我总共完成了四项任务，包括大数据新人学习计划、吉林大数据平台轮播频道统计功能、平遥大数据平台部署任务以及公司基于docker的大数据平台搭建任务。

一、大数据新人学习计划

大数据新人学习计划的内容包括：语言基础、大数据框架以及目前公司的大数据业务三个方面。

1. 语言基础学习

语言基础部分涉及到scala、sql、linux常用命令、shell脚本等，学习的内容包括各个语言的基础语法、常用数据类型等。同时也学习了公司常用的开发工具，包括idea、secureCRT以及如何使用SVN等。同时，我参与了homed组内关于linux的常用命令和服务器相关内容的培训。

2. 大数据框架学习

大数据框架方面包括hadoop、spark、hbase和phoenix。学习的内容包括常用api、框架的运行原理、安装方法、任务调度过程、如何设置框架常用的配置文件以及框架的安装部署等内容。

3. 大数据业务学习

公司的大数据业务方面，包括公司业务逻辑分析和代码逻辑分析两部分。

(1) 公司业务逻辑分析

熟悉了公司大数据集群目前的分布情况以及公司的重点业务表。然后对比统计了淄博大数据平台和公司目前最新版的大数据平台之间在页面逻辑上的差异。目前大数据系统共有 “实时在线”，“设备统计”，“终端统计”，“业务营收”，“统计报表”，“运营统计”，“用户管理”，“系统设置”，用户画像“九个部分。淄博的大数据系统由于版本比较早，所以和新版的大数据系统存在比较大的差异。主要集中在终端统计和统计报表两个部分。终端统计部分相比淄博的早期版本，公司目前的版本有一些内容的调整，例如在“点播统计”中增加了“更多”选项，在该选项中增加了很多报表。“统计报表”部分删减了一些报表，然后新版大数据系统增加了“运营统计”的部分，该部分内容包括了“互动活跃度报表”等相关统计表格。除此之外，我对比分析了淄博大数据系统和公司新版大数据系统运行命令的shell脚本，对比分析了命令运行时配置参数的不同，加载的类的不同。新版大数据系统在早期系统的基础上又增加了很多新的任务，如“栏目点播统计”以及“套餐”等。

(2) 公司代码逻辑分析

代码方面，重点分析了“用户总量计算”和“开机用户”两个模块的代码逻辑。对代码中各个方法的逻辑进行了梳理分析，从程序主类入口开始，按方法的调用顺序，分析每个方法关联的数据，以及输入输出。

再以上三个方面的学习过程中，我也收获了很多，也遇到了很多的问题，这些最终我都以文本的形式记录了下来。



二、吉林大数据平台轮播频道统计功能开发

完成了大数据新人学习计划之后，组长就安排我开始上手公司大数据业务。首先接到的任务是在“终端统计”板块下的“频道统计”内轮播频道统计功能的开发。

该功能需要实现的功能是，新增频道分类(f_channel_category)字段，然后在该字段的基础上实现按频道分类进行数据统计。同时，该统计方法内原有的统计逻辑需要保留。

该任务有以下几个方面的难点：

1. 我对公司目前的业务代码不熟悉
2. 新增一个字段涉及到修改基础表结构，公司内部时通过phoenix在操作hbase的，对于phoenix我是第一次使用
3. 如何在不影响公司原有业务的情况下开发新业务

针对以上几个方面的难点，我首先利用udf这种对原有代码侵入性比较小的方式进行开发，这种方式是基于scala语言进行开发的。这种方式在小规模系统中可以正常运行，但是在大规模集群中由于涉及到广播变量，最终碰到了很多问题，经过和组长讨论之后决定放弃，转而使用sql语言直接进行业务开发。因为纯sql开发对公司原有业务代码的入侵性比较强，所以我对原有的部分代码进行了重构，并对一些废弃的代码进行了整理。

在这项任务开发的过程中，李组长给了我很大的帮助，包括对我进行代码业务逻辑的讲解，对基础组件的使用，还有对我的代码进行评审，以及帮助我与组内其他同事进行沟通。最终这项任务按时交付，目前已经在吉林上线运行了。

此外，在这项开发任务中，我对于saprk中rdd的一些常用api掌握的更加熟练了，对spark程序的开发有了深入的了解，同时对spark的数据处理流程有了更深入的了解。在对代码进行测试的过程中，我编写了执行任务的shell脚本，对于常用的shell语法也有了更深入的了解。同时该任务涉及到了与web端的配合，这也锻炼了我的团队合作能力。



三、平遥大数据平台部署

完成了吉林的任务之后，我接手了平遥大数据系统的部署工作。

该任务主要是平遥现场集群上部署公司最新的大数据平台。该任务的难点主要在于：

1. 我对公司的人员架构不熟悉，不清楚总部和各地方现场之间的关系
2. 我对公司运维人员以及山西同事之间不熟悉，不知道该如何沟通

针对以上两个难点，我积极和李组长沟通，搞清楚了自己的职责，并且和运维以及平遥的同事积极沟通，最终设置好了配置文件，完成了平遥的大数据平台的部署。



四、公司基于docker的大数据平台部署

完成平遥大数据平台的部署工作之后，我来到程序开发二十三组，接手了公司基于docker的大数据平台搭建工作。

该工作的主要内容是，在多台主机上安装docker，并在此基础上安装分布式hadoop集群、spark集群以及hive数仓工具，然后需要把宿主机集群中的数据迁移到docker容器集群中。该任务的难点主要在于：

1. 这是公司内部首次运行docker，我本人对于docker也不是特别熟悉，因此属于一边学习一边工作的状态。
2. 该任务涉及到容器技术、计算机网络、操作系统、shell脚本等多方面的知识，综合能力要求比较高。
3. 在这个任务中我遇到了很多比较难以解决的细节问题，包括docker容器之间的跨主机通信，docker占用宿主机的资源管理，sshd、mysqld等守护进程的启动等。



针对以上难点，我大量的查阅了docker官方文档、hadoop官网等资料，将该任务涉及到的各项计算机基础知识回顾了一遍，并且和杜组长积极交流，最终完成了在多台宿主机上部署docker并搭建大数据环境的任务，该集群已经在公司内部正式开始运营了。

同时，我实现了docker容器集群从0到完整系统的全自动一键式部署功能。实现了hive集群数据的定时增量迁移任务。



以上的四个任务都让我学习到了很多，第一个任务让我重新回顾了整个大数据领域涉及到的相关知识，为我进行后面的任务打下了良好的基础。



第一个任务涉及到和分析公司现有代码的业务逻辑，这有助于我在进入公司业务开发之前先熟悉整个代码流程



在试用期的这四个月之内，我收获了很多，成长了很多。经过四个任务的历练，

公司方面，我对公司的架构，整个业务流程都有了更深入的认识。对公司的总部和现场之间的关系有了初步的认识，对公司运维、DBA等其他部门的同事有了一些交流。了解了大公司内部，各个业务部门之间是如何进行协调分工的；如何与其他部门同事进行有效的沟通，同时也在工作的过程中养成了记录文档的习惯，上述的四项任务中，我对于工作中遇到的问题，学到的知识都进行了总结，用来在任务结束时进行复盘。

专业知识方面，我对于saprk中rdd的一些常用api掌握的更加熟练了，对spark程序的开发有了深入的了解，同时对spark的数据处理流程有了更深入的了解。在对代码进行测试的过程中，我编写了执行任务的shell脚本，对于常用的shell语法也有了更深入的了解。同时也对docker的系统架构有了更深入的认识。

同时，通过这四个月的工作，我也认识到了自己身上还存在很多不足，首先，我对于计算机网络和操作系统的基础知识掌握的还不够，集中体现在我在第四个任务中，给docker容器配置跨集群通信的过程中，基础知识的不足导致我在这方面花费了大量的时间。

我对于公司大数据业务的熟练度掌握的还不够，对目前公司集群分布情况还不是十分的了解，集中体现在搭建docker集群时，对于hdfsslave2这台主机的异常没有及时发现。

我对于公司其他部门人员还不是十分的熟悉，在工作中进行交流的时候并不能非常准确的描述目前遇到的问题，这也会花费掉很多的时间。

针对以上不足，之后我会加强自己在计算机基础学科方面的知识储备，多和其他人员进行沟通，多参加公司的集体活动，多和公司其他人员进行交流，同时提升自己的表达能力，进一步提升自己的工作效率。









